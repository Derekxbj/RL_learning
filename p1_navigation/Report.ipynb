{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Reinforcement Learning : Navigation\n",
    "\n",
    "In this project repository, I detail my results for the Project 1: Navigation.\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "In this project, two *value-based* method, deep Q-learning with experience repaly and Doulbe Q-learning are implemented to achieve the goal.\n",
    "\n",
    "### Deep Q-learning\n",
    "Deep Q-learning combine the Q-learning method (SARSA Max) and the deep neural network to represent the Q-table for approximation. However, the naive Deep-Q-Learning has two main problems:\n",
    " - It will only learn from a single example, and this learning is effectively discarded each time. However, out function approximator is deep neural network, which is a high dimensional space. This make the training tends to be caught in a local minimum finally.\n",
    " - It using the same network to evaluate the value and the maximum action as well when choosing the maximum action, which will change the correlations between them.\n",
    " \n",
    "These problems were solved by the work of DeepMind publication: [\"Human-level control through deep reinforcement learning\"](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf). The two key ideas of DQN are:\n",
    " - Experience replay: removing correlations in the observation sequences and smoothing over changes in the data distribution\n",
    " - Q targets that are only periodically updates: reducing correlations with the target \n",
    "\n",
    "The pseudo code of DQN algorithm from [\"Human-level control through deep reinforcement learning\"](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf) is shown as below.\n",
    "![Deep Q-Learning algorithm](./images/DQN.png)\n",
    "\n",
    "The architecure and the chosen parameters for the deep Q network are shown as follows:\n",
    "![Deep Q-Network](./images/Qnetwork.png)\n",
    "\n",
    "The implementation of the DQN is shown in `dqn_agent.py` of the source code.\n",
    "\n",
    "### Double Q-learning\n",
    "\n",
    "Although DQN show promising performance, [study](https://arxiv.org/pdf/1509.06461.pdf) show DQN more likely to select overestimated values, resulting in overoptimistic value estimates. To tackle this, in Double Q-learning (DDQN), two value functions are learned by assigning each experience randomly to update one of the two value functions, such that there are two sets of weights, $\\theta$ and $\\theta'$. For each update, one set of weights is used to determine the greedy policy and the other to determine its value. For comparison, the learning erros of DQN and DDQN are compared as below: \n",
    "![Deep Q-Learning error](./images/DQNerror.png)\n",
    "![Doulbe deep Q-Learning error](./images/DDQNerror.png)\n",
    "\n",
    "The implementation of the DDQN is shown in `ddqn_agent.py` of the source code.\n",
    "\n",
    "## Hyperparameters\n",
    "The chosen hyperparameters for both agents are shown as below:\n",
    "```python\n",
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters\n",
    "LR = 5e-4               # learning rate \n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "```\n",
    "\n",
    "\n",
    "## Plot of Rewards\n",
    "The results of DQN are shown as below:\n",
    "![Deep Q-Learning training](./images/DQNtraining.png)\n",
    "![Deep Q-Learning results](./images/DQNresults.png)\n",
    "\n",
    "The results of DDQN are shown as below:\n",
    "![DDeep Q-Learning training](./images/DDQNtraining.png)\n",
    "![DDeep Q-Learning results](./images/DDQNresults.png)\n",
    "\n",
    "\n",
    "## Ideas for Future Work\n",
    "\n",
    "For the future work, the performance of the agent can be improved by introducing:\n",
    " - Dueling DQN\n",
    " - Prioritized experience replay"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
