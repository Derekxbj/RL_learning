{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class OUActionNoise():\n",
    "    def __init__(self, mu, sigma=0.15, theta=0.2, dt=1e-2, x0=None):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dt\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def __call__(self):\n",
    "        # noise = QUActionNoise()\n",
    "        # current_noise = noise()\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, max_size, input_shape, n_actions):\n",
    "        self.mem_size = max_size\n",
    "        self.mem_cntr = 0\n",
    "        self.state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.new_state_memory = np.zeros((self.mem_size, *input_shape))\n",
    "        self.action_memory = np.zeros((self.mem_size, n_actions))\n",
    "        self.reward_memory = np.zeros(self.mem_size)\n",
    "        self.terminal_memory = np.zeros(self.mem_size, dtype=np.bool)\n",
    "\n",
    "    def store_transition(self, state, action, reward, state_, done):\n",
    "        index = self.mem_cntr % self.mem_size\n",
    "        self.state_memory[index] = state\n",
    "        self.action_memory[index] = action\n",
    "        self.reward_memory[index] = reward\n",
    "        self.new_state_memory[index] = state_\n",
    "        self.terminal_memory[index] = done\n",
    "\n",
    "        self.mem_cntr += 1\n",
    "\n",
    "    def sample_buffer(self, batch_size):\n",
    "        max_mem = min(self.mem_cntr, self.mem_size)\n",
    "\n",
    "        batch = np.random.choice(max_mem, batch_size)\n",
    "\n",
    "        states = self.state_memory[batch]\n",
    "        actions = self.action_memory[batch]\n",
    "        rewards = self.reward_memory[batch]\n",
    "        states_ = self.new_state_memory[batch]\n",
    "        dones = self.terminal_memory[batch]\n",
    "\n",
    "        return states, actions, rewards, states_, dones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Critic Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, beta, input_dims, fc1_dims, fc2_dims, n_actions, name,\n",
    "                 chkpt_dir='tmp/ddpg'):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.name = name\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_ddpg')\n",
    "\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "\n",
    "        self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
    "        self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
    "        #self.bn1 = nn.BatchNorm1d(self.fc1_dims)\n",
    "        #self.bn2 = nn.BatchNorm1d(self.fc2_dims)\n",
    "\n",
    "        self.action_value = nn.Linear(self.n_actions, self.fc2_dims)\n",
    "        \n",
    "        self.q = nn.Linear(self.fc2_dims, 1)\n",
    "        \n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        self.fc1.weight.data.uniform_(-f1, f1)\n",
    "        self.fc1.bias.data.uniform_(-f1, f1)\n",
    "\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        self.fc2.weight.data.uniform_(-f2, f2)\n",
    "        self.fc2.bias.data.uniform_(-f2, f2)\n",
    "\n",
    "        f3 = 0.003\n",
    "        self.q.weight.data.uniform_(-f3, f3)\n",
    "        self.q.bias.data.uniform_(-f3, f3)\n",
    "\n",
    "        f4 = 1./np.sqrt(self.action_value.weight.data.size()[0])\n",
    "        self.action_value.weight.data.uniform_(-f4, f4)\n",
    "        self.action_value.bias.data.uniform_(-f4, f4)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=beta,\n",
    "                                    weight_decay=0.01)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        state_value = self.fc1(state)\n",
    "        state_value = self.bn1(state_value)\n",
    "        state_value = F.relu(state_value)\n",
    "        state_value = self.fc2(state_value)\n",
    "        state_value = self.bn2(state_value)\n",
    "        #state_value = F.relu(state_value)\n",
    "        #action_value = F.relu(self.action_value(action))\n",
    "        action_value = self.action_value(action)\n",
    "        state_action_value = F.relu(T.add(state_value, action_value))\n",
    "        #state_action_value = T.add(state_value, action_value)\n",
    "        state_action_value = self.q(state_action_value)\n",
    "\n",
    "        return state_action_value\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "    def save_best(self):\n",
    "        print('... saving best checkpoint ...')\n",
    "        checkpoint_file = os.path.join(self.checkpoint_dir, self.name+'_best')\n",
    "        T.save(self.state_dict(), checkpoint_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, alpha, input_dims, fc1_dims, fc2_dims, n_actions, name,\n",
    "                 chkpt_dir='tmp/ddpg'):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.input_dims = input_dims\n",
    "        self.fc1_dims = fc1_dims\n",
    "        self.fc2_dims = fc2_dims\n",
    "        self.n_actions = n_actions\n",
    "        self.name = name\n",
    "        self.checkpoint_dir = chkpt_dir\n",
    "        self.checkpoint_file = os.path.join(self.checkpoint_dir, name+'_ddpg')\n",
    "\n",
    "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\n",
    "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\n",
    "\n",
    "        self.bn1 = nn.LayerNorm(self.fc1_dims)\n",
    "        self.bn2 = nn.LayerNorm(self.fc2_dims)\n",
    "\n",
    "        #self.bn1 = nn.BatchNorm1d(self.fc1_dims)\n",
    "        #self.bn2 = nn.BatchNorm1d(self.fc2_dims)\n",
    "\n",
    "        self.mu = nn.Linear(self.fc2_dims, self.n_actions)\n",
    "\n",
    "        f2 = 1./np.sqrt(self.fc2.weight.data.size()[0])\n",
    "        self.fc2.weight.data.uniform_(-f2, f2)\n",
    "        self.fc2.bias.data.uniform_(-f2, f2)\n",
    "\n",
    "        f1 = 1./np.sqrt(self.fc1.weight.data.size()[0])\n",
    "        self.fc1.weight.data.uniform_(-f1, f1)\n",
    "        self.fc1.bias.data.uniform_(-f1, f1)\n",
    "\n",
    "        f3 = 0.003\n",
    "        self.mu.weight.data.uniform_(-f3, f3)\n",
    "        self.mu.bias.data.uniform_(-f3, f3)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=alpha)\n",
    "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cuda:1')\n",
    "\n",
    "        self.to(self.device)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.fc1(state)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = T.tanh(self.mu(x))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        T.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(T.load(self.checkpoint_file))\n",
    "\n",
    "    def save_best(self):\n",
    "        print('... saving best checkpoint ...')\n",
    "        checkpoint_file = os.path.join(self.checkpoint_dir, self.name+'_best')\n",
    "        T.save(self.state_dict(), checkpoint_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DDPG Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch as T\n",
    "import torch.nn.functional as F\n",
    "# from networks import ActorNetwork, CriticNetwork\n",
    "# from noise import OUActionNoise\n",
    "# from buffer import ReplayBuffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    def __init__(self, alpha, beta, input_dims, tau, n_actions, gamma=0.99,\n",
    "                 max_size=1000000, fc1_dims=400, fc2_dims=300, \n",
    "                 batch_size=64):\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.batch_size = batch_size\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "\n",
    "        self.memory = ReplayBuffer(max_size, input_dims, n_actions)\n",
    "\n",
    "        self.noise = OUActionNoise(mu=np.zeros(n_actions))\n",
    "\n",
    "        self.actor = ActorNetwork(alpha, input_dims, fc1_dims, fc2_dims,\n",
    "                                n_actions=n_actions, name='actor')\n",
    "        self.critic = CriticNetwork(beta, input_dims, fc1_dims, fc2_dims,\n",
    "                                n_actions=n_actions, name='critic')\n",
    "\n",
    "        self.target_actor = ActorNetwork(alpha, input_dims, fc1_dims, fc2_dims,\n",
    "                                n_actions=n_actions, name='target_actor')\n",
    "\n",
    "        self.target_critic = CriticNetwork(beta, input_dims, fc1_dims, fc2_dims,\n",
    "                                n_actions=n_actions, name='target_critic')\n",
    "\n",
    "        self.update_network_parameters(tau=1)\n",
    "\n",
    "    def choose_action(self, observation): # observation of current state\n",
    "        self.actor.eval() # because using layer norm, avoid calculating statistics\n",
    "        state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
    "        mu = self.actor.forward(state).to(self.actor.device)\n",
    "        mu_prime = mu + T.tensor(self.noise(), \n",
    "                                    dtype=T.float).to(self.actor.device)\n",
    "        self.actor.train()\n",
    "\n",
    "        return mu_prime.cpu().detach().numpy()[0]\n",
    "\n",
    "    def remember(self, state, action, reward, state_, done):\n",
    "        self.memory.store_transition(state, action, reward, state_, done)\n",
    "\n",
    "    def save_models(self):\n",
    "        self.actor.save_checkpoint()\n",
    "        self.target_actor.save_checkpoint()\n",
    "        self.critic.save_checkpoint()\n",
    "        self.target_critic.save_checkpoint()\n",
    "\n",
    "    def load_models(self):\n",
    "        self.actor.load_checkpoint()\n",
    "        self.target_actor.load_checkpoint()\n",
    "        self.critic.load_checkpoint()\n",
    "        self.target_critic.load_checkpoint()\n",
    "\n",
    "    def learn(self):\n",
    "        if self.memory.mem_cntr < self.batch_size: # go back to main loop if less than batch size\n",
    "            return\n",
    "\n",
    "        states, actions, rewards, states_, done = \\\n",
    "                self.memory.sample_buffer(self.batch_size)\n",
    "\n",
    "        states = T.tensor(states, dtype=T.float).to(self.actor.device)\n",
    "        states_ = T.tensor(states_, dtype=T.float).to(self.actor.device)\n",
    "        actions = T.tensor(actions, dtype=T.float).to(self.actor.device)\n",
    "        rewards = T.tensor(rewards, dtype=T.float).to(self.actor.device)\n",
    "        done = T.tensor(done).to(self.actor.device)\n",
    "        \n",
    "        # yi=ri+γ Q' (si+1 ,μ ' (si+1|θ μ ' )|θ Q' )\n",
    "        target_actions = self.target_actor.forward(states_)\n",
    "        critic_value_ = self.target_critic.forward(states_, target_actions)\n",
    "        critic_value = self.critic.forward(states, actions)\n",
    "        \n",
    "        critic_value_[done] = 0.0\n",
    "        critic_value_ = critic_value_.view(-1)\n",
    "\n",
    "        target = rewards + self.gamma*critic_value_\n",
    "        target = target.view(self.batch_size, 1)\n",
    "\n",
    "        self.critic.optimizer.zero_grad()\n",
    "        critic_loss = F.mse_loss(target, critic_value)\n",
    "        critic_loss.backward()\n",
    "        self.critic.optimizer.step()\n",
    "\n",
    "        self.actor.optimizer.zero_grad()\n",
    "        actor_loss = -self.critic.forward(states, self.actor.forward(states))\n",
    "        actor_loss = T.mean(actor_loss)\n",
    "        actor_loss.backward()\n",
    "        self.actor.optimizer.step()\n",
    "\n",
    "        self.update_network_parameters()\n",
    "\n",
    "    def update_network_parameters(self, tau=None):\n",
    "        if tau is None:\n",
    "            tau = self.tau\n",
    "\n",
    "        actor_params = self.actor.named_parameters()\n",
    "        critic_params = self.critic.named_parameters()\n",
    "        target_actor_params = self.target_actor.named_parameters()\n",
    "        target_critic_params = self.target_critic.named_parameters()\n",
    "\n",
    "        critic_state_dict = dict(critic_params)\n",
    "        actor_state_dict = dict(actor_params)\n",
    "        target_critic_state_dict = dict(target_critic_params)\n",
    "        target_actor_state_dict = dict(target_actor_params)\n",
    "\n",
    "        for name in critic_state_dict:\n",
    "            critic_state_dict[name] = tau*critic_state_dict[name].clone() + \\\n",
    "                                (1-tau)*target_critic_state_dict[name].clone()\n",
    "\n",
    "        for name in actor_state_dict:\n",
    "             actor_state_dict[name] = tau*actor_state_dict[name].clone() + \\\n",
    "                                 (1-tau)*target_actor_state_dict[name].clone()\n",
    "\n",
    "        self.target_critic.load_state_dict(critic_state_dict)\n",
    "        self.target_actor.load_state_dict(actor_state_dict)\n",
    "        #self.target_critic.load_state_dict(critic_state_dict, strict=False)\n",
    "        #self.target_actor.load_state_dict(actor_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "# from ddpg_torch import Agent\n",
    "# from utils import plot_learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_learning_curve(x, scores, figure_file):\n",
    "    running_avg = np.zeros(len(scores))\n",
    "    for i in range(len(running_avg)):\n",
    "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\n",
    "    plt.plot(x, running_avg)\n",
    "    plt.title('Running average of previous 100 scores')\n",
    "    plt.savefig(figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  0 score -238.7 average score -238.7\n",
      "episode  1 score -466.4 average score -352.6\n",
      "episode  2 score -248.5 average score -317.9\n",
      "episode  3 score -384.1 average score -334.4\n",
      "episode  4 score -379.3 average score -343.4\n",
      "episode  5 score -902.7 average score -436.6\n",
      "episode  6 score -427.9 average score -435.4\n",
      "episode  7 score -566.3 average score -451.8\n",
      "episode  8 score -484.5 average score -455.4\n",
      "episode  9 score -477.2 average score -457.6\n",
      "episode  10 score -692.6 average score -478.9\n",
      "episode  11 score -480.5 average score -479.1\n",
      "episode  12 score -147.2 average score -453.5\n",
      "episode  13 score -731.3 average score -473.4\n",
      "episode  14 score -1562.1 average score -546.0\n",
      "episode  15 score -380.6 average score -535.6\n",
      "episode  16 score -509.8 average score -534.1\n",
      "episode  17 score -634.5 average score -539.7\n",
      "episode  18 score -797.2 average score -553.2\n",
      "episode  19 score -234.5 average score -537.3\n",
      "episode  20 score -635.6 average score -542.0\n",
      "episode  21 score -619.3 average score -545.5\n",
      "episode  22 score -1033.3 average score -566.7\n",
      "episode  23 score -940.2 average score -582.3\n",
      "episode  24 score -1146.5 average score -604.8\n",
      "episode  25 score -366.3 average score -595.7\n",
      "episode  26 score -223.1 average score -581.9\n",
      "episode  27 score -273.7 average score -570.9\n",
      "episode  28 score -288.5 average score -561.1\n",
      "episode  29 score -278.2 average score -551.7\n",
      "episode  30 score -250.6 average score -542.0\n",
      "episode  31 score -489.3 average score -540.3\n",
      "episode  32 score -289.0 average score -532.7\n",
      "episode  33 score -555.0 average score -533.4\n",
      "episode  34 score -222.5 average score -524.5\n",
      "episode  35 score -228.3 average score -516.3\n",
      "episode  36 score -113.4 average score -505.4\n",
      "episode  37 score -295.0 average score -499.8\n",
      "episode  38 score -223.5 average score -492.8\n",
      "episode  39 score -155.0 average score -484.3\n",
      "episode  40 score -178.3 average score -476.8\n",
      "episode  41 score -180.5 average score -469.8\n",
      "episode  42 score -18.6 average score -459.3\n",
      "episode  43 score -140.9 average score -452.1\n",
      "episode  44 score -295.3 average score -448.6\n",
      "episode  45 score -36.9 average score -439.6\n",
      "episode  46 score -305.5 average score -436.8\n",
      "episode  47 score -224.4 average score -432.3\n",
      "episode  48 score -152.8 average score -426.6\n",
      "episode  49 score -180.2 average score -421.7\n",
      "episode  50 score -217.8 average score -417.7\n",
      "episode  51 score -143.2 average score -412.4\n",
      "episode  52 score -125.5 average score -407.0\n",
      "episode  53 score -182.3 average score -402.9\n",
      "episode  54 score -210.5 average score -399.4\n",
      "episode  55 score -173.0 average score -395.3\n",
      "episode  56 score -157.6 average score -391.1\n",
      "episode  57 score -127.2 average score -386.6\n",
      "episode  58 score -262.3 average score -384.5\n",
      "episode  59 score -171.4 average score -380.9\n",
      "episode  60 score -424.1 average score -381.6\n",
      "episode  61 score -384.9 average score -381.7\n",
      "episode  62 score -320.5 average score -380.7\n",
      "episode  63 score -62.8 average score -375.8\n",
      "episode  64 score -149.0 average score -372.3\n",
      "episode  65 score -310.5 average score -371.3\n",
      "episode  66 score -176.8 average score -368.4\n",
      "episode  67 score -128.6 average score -364.9\n",
      "episode  68 score -425.5 average score -365.8\n",
      "episode  69 score -222.4 average score -363.7\n",
      "episode  70 score -127.0 average score -360.4\n",
      "episode  71 score -99.1 average score -356.8\n",
      "episode  72 score -55.9 average score -352.6\n",
      "episode  73 score -494.8 average score -354.6\n",
      "episode  74 score -132.3 average score -351.6\n",
      "episode  75 score -301.3 average score -350.9\n",
      "episode  76 score -137.0 average score -348.2\n",
      "episode  77 score -244.4 average score -346.8\n",
      "episode  78 score -163.5 average score -344.5\n",
      "episode  79 score -243.5 average score -343.3\n",
      "episode  80 score -154.6 average score -340.9\n",
      "episode  81 score -153.1 average score -338.6\n",
      "episode  82 score -139.4 average score -336.2\n",
      "episode  83 score -251.4 average score -335.2\n",
      "episode  84 score -115.0 average score -332.6\n",
      "episode  85 score -228.2 average score -331.4\n",
      "episode  86 score -245.6 average score -330.4\n",
      "episode  87 score 22.3 average score -326.4\n",
      "episode  88 score -175.7 average score -324.7\n",
      "episode  89 score -375.8 average score -325.3\n",
      "episode  90 score -157.8 average score -323.5\n",
      "episode  91 score -20.8 average score -320.2\n",
      "episode  92 score -183.4 average score -318.7\n",
      "episode  93 score -395.0 average score -319.5\n",
      "episode  94 score -194.4 average score -318.2\n",
      "episode  95 score -65.4 average score -315.6\n",
      "episode  96 score -203.6 average score -314.4\n",
      "episode  97 score -193.0 average score -313.2\n",
      "episode  98 score -180.0 average score -311.8\n",
      "episode  99 score -162.6 average score -310.3\n",
      "episode  100 score -387.4 average score -311.8\n",
      "episode  101 score -199.6 average score -309.1\n",
      "episode  102 score -198.0 average score -308.6\n",
      "episode  103 score -90.0 average score -305.7\n",
      "episode  104 score -169.2 average score -303.6\n",
      "episode  105 score -251.8 average score -297.1\n",
      "episode  106 score -225.8 average score -295.1\n",
      "episode  107 score -135.3 average score -290.8\n",
      "episode  108 score -181.7 average score -287.7\n",
      "episode  109 score -268.9 average score -285.6\n",
      "episode  110 score -234.1 average score -281.1\n",
      "episode  111 score -387.0 average score -280.1\n",
      "episode  112 score -220.4 average score -280.9\n",
      "episode  113 score -190.4 average score -275.4\n",
      "episode  114 score -183.7 average score -261.7\n",
      "episode  115 score -304.4 average score -260.9\n",
      "episode  116 score -447.7 average score -260.3\n",
      "episode  117 score -184.0 average score -255.8\n",
      "episode  118 score -199.5 average score -249.8\n",
      "episode  119 score -237.8 average score -249.8\n",
      "episode  120 score -208.2 average score -245.6\n",
      "episode  121 score -213.1 average score -241.5\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  122 score -212.7 average score -233.3\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  123 score -139.7 average score -225.3\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  124 score -93.7 average score -214.8\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  125 score -198.0 average score -213.1\n",
      "episode  126 score -340.0 average score -214.2\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  127 score -118.9 average score -212.7\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  128 score -120.8 average score -211.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  129 score -117.1 average score -209.4\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  130 score -220.2 average score -209.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  131 score -80.4 average score -205.0\n",
      "episode  132 score -348.3 average score -205.6\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  133 score -104.3 average score -201.1\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  134 score -177.4 average score -200.7\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  135 score -111.4 average score -199.5\n",
      "episode  136 score -299.6 average score -201.3\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  137 score -95.3 average score -199.3\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  138 score -130.4 average score -198.4\n",
      "episode  139 score -186.4 average score -198.7\n",
      "episode  140 score -434.9 average score -201.3\n",
      "episode  141 score -161.3 average score -201.1\n",
      "episode  142 score -86.6 average score -201.8\n",
      "episode  143 score -249.5 average score -202.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode  144 score -137.6 average score -201.3\n",
      "episode  145 score -267.4 average score -203.6\n",
      "episode  146 score -134.0 average score -201.9\n",
      "episode  147 score -101.0 average score -200.7\n",
      "episode  148 score -127.4 average score -200.4\n",
      "episode  149 score -81.9 average score -199.4\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  150 score -68.0 average score -197.9\n",
      "episode  151 score -318.7 average score -199.7\n",
      "episode  152 score -97.3 average score -199.4\n",
      "episode  153 score -119.8 average score -198.8\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  154 score -61.5 average score -197.3\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  155 score -95.4 average score -196.5\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  156 score -51.4 average score -195.4\n",
      "episode  157 score -177.6 average score -195.9\n",
      "episode  158 score -1082.1 average score -204.1\n",
      "episode  159 score -86.1 average score -203.3\n",
      "episode  160 score -85.8 average score -199.9\n",
      "episode  161 score -191.9 average score -198.0\n",
      "episode  162 score -117.5 average score -195.9\n",
      "episode  163 score -96.6 average score -196.3\n",
      "episode  164 score -92.8 average score -195.7\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  165 score -81.9 average score -193.4\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  166 score -74.2 average score -192.4\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  167 score -50.8 average score -191.6\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  168 score -84.1 average score -188.2\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  169 score 138.3 average score -184.6\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  170 score 103.7 average score -182.3\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  171 score 105.9 average score -180.3\n",
      "episode  172 score -69.0 average score -180.4\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  173 score 168.5 average score -173.8\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "episode  174 score 60.3 average score -171.8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5231ca04d8b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mobservation_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[0mscore\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-87ac52b4061e>\u001b[0m in \u001b[0;36mlearn\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[0mtarget_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_actor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[0mcritic_value_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_critic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m         \u001b[0mcritic_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstates\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mcritic_value_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdone\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-fb48cf70ecd4>\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, state, action)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[1;31m#action_value = F.relu(self.action_value(action))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0maction_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 56\u001b[1;33m         \u001b[0mstate_action_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     57\u001b[0m         \u001b[1;31m#state_action_value = T.add(state_value, action_value)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m         \u001b[0mstate_action_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_action_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "agent = Agent(alpha=0.0001, beta=0.001, \n",
    "                input_dims=env.observation_space.shape, tau=0.001,\n",
    "                batch_size=64, fc1_dims=400, fc2_dims=300, \n",
    "                n_actions=env.action_space.shape[0])\n",
    "n_games = 1000\n",
    "filename = 'LunarLander_alpha_' + str(agent.alpha) + '_beta_' + \\\n",
    "            str(agent.beta) + '_' + str(n_games) + '_games'\n",
    "figure_file = 'plots/' + filename + '.png'\n",
    "\n",
    "best_score = env.reward_range[0]\n",
    "score_history = []\n",
    "for i in range(n_games):\n",
    "    observation = env.reset()\n",
    "    done = False\n",
    "    score = 0\n",
    "    agent.noise.reset()\n",
    "    while not done:\n",
    "        action = agent.choose_action(observation)\n",
    "        observation_, reward, done, info = env.step(action)\n",
    "        agent.remember(observation, action, reward, observation_, done)\n",
    "        agent.learn()\n",
    "        score += reward\n",
    "        observation = observation_\n",
    "    score_history.append(score)\n",
    "    avg_score = np.mean(score_history[-100:])\n",
    "\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        agent.save_models()\n",
    "\n",
    "    print('episode ', i, 'score %.1f' % score,\n",
    "            'average score %.1f' % avg_score)\n",
    "x = [i+1 for i in range(n_games)]\n",
    "plot_learning_curve(x, score_history, figure_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
